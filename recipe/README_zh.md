# Recipe（配方）
`recipes/` 下的示例是 verl 针对特定端到端 RL 训练配方的代表性扩展。
为了帮助社区重现实验，verl 团队提供了每个配方最初 PR 到 verl main 时代码库的快照。您可以通过 [github 分支](https://github.com/volcengine/verl/branches/all?query=recipe) 找到它们

# 使用 verl 的优秀作品

- [Logic-RL](https://github.com/Unakar/Logic-RL)：在 2K 微小逻辑谜题数据集上重现 DeepSeek R1 Zero。![GitHub Repo stars](https://img.shields.io/github/stars/Unakar/Logic-RL)
- [Seed-Coder](https://github.com/ByteDance-Seed/Seed-Coder)：Seed-Coder 的 RL 训练提升了竞赛编程的性能 ![GitHub Repo stars](https://img.shields.io/github/stars/ByteDance-Seed/Seed-Coder)
- [all-hands/openhands-lm-32b-v0.1](https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model)：一个强大的开源编程代理模型，通过[多轮微调](https://github.com/volcengine/verl/pull/195)训练
- [s3](https://github.com/pat-jj/s3) 通过 RL 进行**高效而有效**的搜索代理训练 ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/s3)
- [Rec-R1](https://arxiv.org/pdf/2503.24289)：通过强化学习连接生成大语言模型和推荐系统
- [Explore RL Data Scaling](https://arxiv.org/abs/2503.22230)：探索人类反馈强化学习中的数据缩放趋势和效果
- [FIRE](https://arxiv.org/abs/2410.21236)：大语言模型的火热启动与常规执行采样
- [DQO](https://arxiv.org/abs/2410.09302)：通过直接 Q 函数优化增强语言模型的多步推理能力
- [ProRL](https://arxiv.org/abs/2505.24864)：持续强化学习扩展大语言模型的推理边界
- [cognition-engineering](https://github.com/gair-nlp/cognition-engineering)：测试时间缩放驱动认知工程 ![GitHub Repo stars](https://img.shields.io/github/stars/gair-nlp/cognition-engineering)
- [Trust Region Preference Approximation](https://github.com/XueruiSu/Trust-Region-Preference-Approximation)：用于 LLM 推理的简单稳定**强化学习算法** ![GitHub Repo stars](https://img.shields.io/github/stars/XueruiSu/Trust-Region-Preference-Approximation)
- [AdaRFT](https://github.com/uscnlp-lime/verl)：通过**自适应课程学习**进行高效强化微调 ![GitHub Repo stars](https://img.shields.io/github/stars/uscnlp-lime/verl)
- [critic-rl](https://github.com/HKUNLP/critic-rl)：用于代码生成的 LLM 评论者 ![GitHub Repo stars](https://img.shields.io/github/stars/HKUNLP/critic-rl)
- [self-rewarding-reasoning-LLM](https://arxiv.org/pdf/2502.19613)：使用**生成奖励模型**进行自我奖励和纠正 ![GitHub Repo stars](https://img.shields.io/github/stars/RLHFlow/Self-rewarding-reasoning-LLM)
- [DeepEnlighten](https://github.com/DolbyUUU/DeepEnlighten)：使用**社会推理**任务重现 R1 并分析关键发现 ![GitHub Repo stars](https://img.shields.io/github/stars/DolbyUUU/DeepEnlighten)
- [MetaSpatial](https://github.com/PzySeere/MetaSpatial)：在**元宇宙**中强化 **VLMs** 的 **3D 空间推理** ![GitHub Repo stars](https://img.shields.io/github/stars/PzySeere/MetaSpatial)
- [PURE](https://github.com/CJReinforce/PURE)：**信用分配**是使用**过程奖励模型**进行成功强化微调的关键 ![GitHub Repo stars](https://img.shields.io/github/stars/CJReinforce/PURE)
- [cognitive-behaviors](https://github.com/kanishkg/cognitive-behaviors)：使自我改进推理者成为可能的认知行为，或者，高效 STaR 的四个习惯 ![GitHub Repo stars](https://img.shields.io/github/stars/kanishkg/cognitive-behaviors)
- [deepscaler](https://github.com/agentica-project/rllm/tree/deepscaler)：使用 GRPO 进行迭代上下文缩放 ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/deepscaler)
- [DAPO](https://dapo-sia.github.io/)：击败 DeepSeek-R1-zero-32B 的完全开源 SOTA RL 算法 ![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)
- [NoisyRollout](https://github.com/NUS-TRAIL/NoisyRollout)：通过数据增强强化视觉推理 ![GitHub Repo stars](https://img.shields.io/github/stars/NUS-TRAIL/NoisyRollout)